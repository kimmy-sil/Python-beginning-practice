{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Getting started with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Basics \n",
    "\n",
    "Python is an interpreted language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.6\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "\n",
    "bibexcel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three commands for accessing packages\n",
    "\n",
    "+ import\n",
    "+ from ... import\n",
    "+ reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute the following code to check the version of Python\n",
    "\n",
    "import sys\n",
    "\n",
    "print(sys.version_info)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you are not sure about the packages\n",
    "* help\n",
    "* dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package pandas:\n",
      "\n",
      "NAME\n",
      "    pandas\n",
      "\n",
      "DESCRIPTION\n",
      "    pandas - a powerful data analysis and manipulation library for Python\n",
      "    =====================================================================\n",
      "    \n",
      "    **pandas** is a Python package providing fast, flexible, and expressive data\n",
      "    structures designed to make working with \"relational\" or \"labeled\" data both\n",
      "    easy and intuitive. It aims to be the fundamental high-level building block for\n",
      "    doing practical, **real world** data analysis in Python. Additionally, it has\n",
      "    the broader goal of becoming **the most powerful and flexible open source data\n",
      "    analysis / manipulation tool available in any language**. It is already well on\n",
      "    its way toward this goal.\n",
      "    \n",
      "    Main Features\n",
      "    -------------\n",
      "    Here are just a few of the things that pandas does well:\n",
      "    \n",
      "      - Easy handling of missing data in floating point as well as non-floating\n",
      "        point data.\n",
      "      - Size mutability: columns can be inserted and deleted from DataFrame and\n",
      "        higher dimensional objects\n",
      "      - Automatic and explicit data alignment: objects can be explicitly aligned\n",
      "        to a set of labels, or the user can simply ignore the labels and let\n",
      "        `Series`, `DataFrame`, etc. automatically align the data for you in\n",
      "        computations.\n",
      "      - Powerful, flexible group by functionality to perform split-apply-combine\n",
      "        operations on data sets, for both aggregating and transforming data.\n",
      "      - Make it easy to convert ragged, differently-indexed data in other Python\n",
      "        and NumPy data structures into DataFrame objects.\n",
      "      - Intelligent label-based slicing, fancy indexing, and subsetting of large\n",
      "        data sets.\n",
      "      - Intuitive merging and joining data sets.\n",
      "      - Flexible reshaping and pivoting of data sets.\n",
      "      - Hierarchical labeling of axes (possible to have multiple labels per tick).\n",
      "      - Robust IO tools for loading data from flat files (CSV and delimited),\n",
      "        Excel files, databases, and saving/loading data from the ultrafast HDF5\n",
      "        format.\n",
      "      - Time series-specific functionality: date range generation and frequency\n",
      "        conversion, moving window statistics, date shifting and lagging.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _config (package)\n",
      "    _libs (package)\n",
      "    _testing\n",
      "    _typing\n",
      "    _version\n",
      "    api (package)\n",
      "    arrays (package)\n",
      "    compat (package)\n",
      "    conftest\n",
      "    core (package)\n",
      "    errors (package)\n",
      "    io (package)\n",
      "    plotting (package)\n",
      "    testing\n",
      "    tests (package)\n",
      "    tseries (package)\n",
      "    util (package)\n",
      "\n",
      "SUBMODULES\n",
      "    _hashtable\n",
      "    _lib\n",
      "    _tslib\n",
      "    offsets\n",
      "\n",
      "FUNCTIONS\n",
      "    __getattr__(name)\n",
      "\n",
      "DATA\n",
      "    IndexSlice = <pandas.core.indexing._IndexSlice object>\n",
      "    NA = <NA>\n",
      "    NaT = NaT\n",
      "    __docformat__ = 'restructuredtext'\n",
      "    __git_version__ = '29d6b0232aab9576afa896ff5bab0b994760495a'\n",
      "    describe_option = <pandas._config.config.CallableDynamicDoc object>\n",
      "    get_option = <pandas._config.config.CallableDynamicDoc object>\n",
      "    options = <pandas._config.config.DictWrapper object>\n",
      "    reset_option = <pandas._config.config.CallableDynamicDoc object>\n",
      "    set_option = <pandas._config.config.CallableDynamicDoc object>\n",
      "\n",
      "VERSION\n",
      "    1.0.1\n",
      "\n",
      "FILE\n",
      "    d:\\anaconda\\lib\\site-packages\\pandas\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "help (pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example  \n",
    "\n",
    "!pip install pandas  \n",
    "!pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "# zen of python\n",
    "\n",
    "'''\n",
    "Long time Pythoneer Tim Peters succinctly channels the BDFL's guiding principles for Python's design into 20 aphorisms, \n",
    "only 19 of which have been written down.\n",
    "'''\n",
    "\n",
    "import this\n",
    "\n",
    "import this\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display sth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print('Hello world')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# print absolute value of an integer:\n",
    "\n",
    "a = 100\n",
    "if a >= 0:\n",
    "    print(a)\n",
    "else:\n",
    "    print(-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical operations\n",
    "\n",
    "* +、-、*、/、**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 + 3 * 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Integers:\t-2, -1, 0, 1, 2, 3, 4, 5\n",
    "+ Floating-point numbers:\t-1.25, -1.0, --0.5, 0.0, 0.5, 1.0, 1.25\n",
    "+ Strings:\t'a', 'aa', 'aaa', 'Hello!', '11 cats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "x = 5\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Literals\n",
    "\n",
    "Strings are sequences or collections of characters that are used to store and represent \n",
    "textual data, which is our data type of choice in most of the book’s examples. \n",
    "\n",
    "There are various types of strings, as mentioned earlier. The following BNF (Backus-Naur \n",
    "Form) gives us the general lexical definitions for producing strings, as seen in the official \n",
    "Python docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Short strings**: These strings are usually enclosed with single quotes (') or double quotes (\") around the characters. Some examples are 'Hello' and \"Hello\".\n",
    "<br/>\n",
    "\n",
    "+ **Long strings**: These strings are usually enclosed with three single  (''') or double quotes (\"\"\") around the characters. Some examples  are \"\"\"Hello, I'm a long string\"\"\" or '''Hello I\\'m a long  string '''. Note the (\\') indicates an escape sequence which we shall talk about soon.\n",
    "<br/>\n",
    "\n",
    "+ **Raw strings**: These strings were originally created specifically for regular expressions (regex) and regex patterns. These strings can be  created using the r'...' notation and keep the string in its raw or  native form. Hence, they do not perform any backspace interpolation and turn off the escape sequences. An example is r'Hello'.\n",
    "<br/>\n",
    "\n",
    "+ **Unicode**: These strings support Unicode characters in text and they are usually non-ASCII character sequences. These strings are denoted with the u'...' notation. However, in Python 3.x all string literals are typically represented as Unicode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = 'world'\n",
    "saying = \"hello world\"\n",
    "paragraph = \"\"\" This is\n",
    "a paragraph\n",
    "\"\"\"\n",
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character -> P has index-> 0\n",
      "Character -> Y has index-> 1\n",
      "Character -> T has index-> 2\n",
      "Character -> H has index-> 3\n",
      "Character -> O has index-> 4\n",
      "Character -> N has index-> 5\n"
     ]
    }
   ],
   "source": [
    "# Indexing and Slicing\n",
    "\n",
    "p = 'PYTHON'\n",
    "for index, character in enumerate(p):\n",
    "    print('Character ->', character, 'has index->', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('P', 'Y', 'T', 'H', 'O', 'N')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0], p[1], p[2], p[3], p[4], p[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PYTHON'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# string slicing\n",
    "p[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YTH'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List\n",
    "\n",
    "Lists are used to store multiple items in a single variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "thislist = [\"apple\", \"banana\", \"cherry\"]\n",
    "print(thislist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'o', 'n', 'd', 'o', 'n', ' ', 'i', 's', ' ', 'a', ' ', 'b', 'i', 'g', ' ', 'c', 'i', 't', 'y', '.']\n"
     ]
    }
   ],
   "source": [
    "sennn=\"London is a big city.\"\n",
    "try_sennn=list(sennn)\n",
    "print(try_sennn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "\n",
    "Dictionaries are used to store data values in key:value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brand': 'Ford', 'model': 'Mustang', 'year': 1964}\n"
     ]
    }
   ],
   "source": [
    "thisdict = {\n",
    "  \"brand\": \"Ford\",\n",
    "  \"model\": \"Mustang\",\n",
    "  \"year\": 1964\n",
    "}\n",
    "print(thisdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More collection types: \n",
    "\n",
    "+ List is a collection which is ordered and changeable. Allows duplicate members.\n",
    "+ Tuple is a collection which is ordered and unchangeable. Allows duplicate members.\n",
    "+ Set is a collection which is unordered, unchangeable*, and unindexed. No duplicate members.\n",
    "+ Dictionary is a collection which is ordered** and changeable. No duplicate members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Opening files with python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open text file\n",
    "\n",
    "You can open a file with different file modes:  \n",
    "\n",
    "w -> write only  \n",
    "\n",
    "r -> read only  \n",
    "\n",
    "w+ -> read and write + completely overwrite file  \n",
    "\n",
    "a+ -> read and write + append at the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open a file for reading it is enough to specify the name of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file, If the file is located in a different location, you will have to specify the file path\n",
    "\n",
    "with open('C:/Users/kimuj/OneDrive/Diary/2021/Dec/test.txt','r', encoding='utf-8')as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "    #file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Python is great. \n",
      "Good luck!\n"
     ]
    }
   ],
   "source": [
    "print(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to an Existing File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/kimuj/OneDrive/Diary/2021/Dec/test.txt', 'w+', encoding='utf-8') as file:\n",
    "    file.write('Learning Python is great. \\nGood luck!\\n Saturday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I am using a with statement when opening files.\n",
    "Another method is to use open and close:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('text_sample.txt'), 'r')\n",
    "file_content = f.read()\n",
    "f.close()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The with method is preferred as it automatically closes the file.\n",
    "This prevents the file from being 'in use' if you forget to use .close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Working with text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'python is great'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python is great'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PYTHON IS GREAT'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python Is Great'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String splitting and joining\n",
    "\n",
    "c = 'I,am,a,comma,separated,string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'a', 'comma', 'separated', 'string']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions, known more popularly as regexes, \n",
    "allow you to create string patterns \n",
    "and use them for searching and substituting specific pattern matches in textual data.\n",
    "\n",
    "Python offers a rich module named re for creating and using regular expressions.\n",
    "\n",
    "re.match(): This method is used to match patterns at the beginning \n",
    "of strings.\n",
    "<br/>\n",
    "\n",
    "re.compile(): This method compiles a specified regular expression \n",
    "pattern into a regular expression object, which can be used for \n",
    "matching and searching. Takes a pattern and optional flags as input, \n",
    "which we discussed previously.\n",
    "<br/>\n",
    "\n",
    "re.findall(): This method returns all non-overlapping matches of \n",
    "the specified regex pattern in the string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[a-zA-Z0-9]：any alphabets and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', 'Fam']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt1 = 'WWe111 Are22 Family3 !'\n",
    "reg = \"[A-Z]+[a-z]{2}[0-9]*?\"\n",
    "\n",
    "parttern = re.compile(reg)\n",
    "content = re.findall(parttern,txt1)\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\spacy\\util.py:833: UserWarning: [W095] Model 'en_core_web_sm' (3.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.2.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# following line is optional for custom vocabulary installation\n",
    "# you can use nlp = spacy.load('en')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown fox is quick and he is jumping over the lazy dog'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lazy', 'over', 'he', 'fox', 'The', 'and', 'is', 'brown', 'dog', 'is', 'the', 'quick', 'jumping']\n"
     ]
    }
   ],
   "source": [
    "words = sentence.split()\n",
    "np.random.shuffle(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>brown</td>\n",
       "      <td>fox</td>\n",
       "      <td>is</td>\n",
       "      <td>quick</td>\n",
       "      <td>and</td>\n",
       "      <td>he</td>\n",
       "      <td>is</td>\n",
       "      <td>jumping</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>lazy</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>PRP</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>VBG</td>\n",
       "      <td>IN</td>\n",
       "      <td>DT</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1    2    3      4    5    6    7        8     9   10    11   12\n",
       "0  The  brown  fox   is  quick  and   he   is  jumping  over  the  lazy  dog\n",
       "1   DT     JJ   NN  VBZ     JJ   CC  PRP  VBZ      VBG    IN   DT    JJ   NN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = nltk.pos_tag(sentence.split())\n",
    "pd.DataFrame(pos_tags).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>brown</td>\n",
       "      <td>fox</td>\n",
       "      <td>is</td>\n",
       "      <td>quick</td>\n",
       "      <td>and</td>\n",
       "      <td>he</td>\n",
       "      <td>is</td>\n",
       "      <td>jumping</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>lazy</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>PRP</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>VBG</td>\n",
       "      <td>IN</td>\n",
       "      <td>DT</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DET</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>AUX</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>PRON</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADP</td>\n",
       "      <td>DET</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1     2    3      4      5     6    7        8     9   10    11  \\\n",
       "0  The  brown   fox   is  quick    and    he   is  jumping  over  the  lazy   \n",
       "1   DT     JJ    NN  VBZ     JJ     CC   PRP  VBZ      VBG    IN   DT    JJ   \n",
       "2  DET    ADJ  NOUN  AUX    ADJ  CCONJ  PRON  AUX     VERB   ADP  DET   ADJ   \n",
       "\n",
       "     12  \n",
       "0   dog  \n",
       "1    NN  \n",
       "2  NOUN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in nlp(sentence)]\n",
    "pd.DataFrame(spacy_pos_tagged).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some frequently used libraries\n",
    "\n",
    "+ numpy is a very handy library that allows for fairly quick computation over matrices and vectors.  \n",
    "\n",
    "+ pandas is built around \"dataframes\", which are helpful data structures for data sets. It allows one to apply various transformations to datasets with ease and relative speed.  \n",
    "\n",
    "+ nltk is a library with a whole bunch of tools for doing natural language processing. It's generally considered a standard in the field, though a lot of reasonable alternatives exist, including Stanza (developed by folks at Stanford) and SpaCy.  \n",
    "\n",
    "+ matplotlib.pyplot and seaborn are both libraries that help with visualizing data in Python. These are definitely not the only packages one could reasonably use (e.g. Altair, Vega-Lite, and Plotly), but I find that these two libraries nicely combine ease of use, quality visualization, and intuitive customizeability.  \n",
    "\n",
    "+ statsmodel is a library that lets you do basic econometric analyses (e.g. multiple linear regression) in Python. In my honest opinion I think this library leaves much to be desired, but I haven't found a better alternative (and am open to suggestions!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "\n",
    "install spaCy under anaconda, use the following line:  \n",
    "\n",
    "conda config  \n",
    "conda install spacy\n",
    "\n",
    "conda install -c conda-forge space-model-en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\spacy\\util.py:833: UserWarning: [W095] Model 'en_core_web_sm' (3.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.2.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will also use the gensim library, for topic models and embeddings, so please make sure you do not get an error in the next cell\n",
    "\n",
    "You can install the gensim and nltk libraries through Anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, please make sure you have nltk installed, and then download the data by executing the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading NLTK\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to Download all packages of NLTK\n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is downloaded to your machine, you can load some of it using the Python interpreter. The first step is to type a special command at the Python prompt which tells the interpreter to load some texts for us to explore: from nltk.book import *. This says \"from NLTK's book module, load all items.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# Introductory Examples for the NLTK Book\n",
    "\n",
    "from nltk.book import *\n",
    "\n",
    "\n",
    "#  This says \"from NLTK's book module, load all items.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the installed data use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method CategorizedCorpusReader.categories of <CategorizedTaggedCorpusReader in 'C:\\\\Users\\\\kimuj\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\brown'>>\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Knowing Language\n",
    "\n",
    "Before we dive into the applications, let’s take a look at the subject we are\n",
    "working with: language\n",
    "\n",
    "Language often encodes information redundantly, i.e., we say the same thing in\n",
    "several ways: through the meanings of the words, their positions, the context,\n",
    "and many other cues\n",
    "\n",
    "Words themselves consist of different components, which\n",
    "are the focus of different linguistic disciplines: their meaning (semantics), their function in a sentence (syntax), and the prefixes and endings (morphology).\n",
    "Not all words have all of this information. \n",
    "\n",
    "And when we work with textual data,\n",
    "we might not be interested in all of this information. In fact, it can be beneficial\n",
    "to remove some of the information we do not need.\n",
    "\n",
    "When we work with text, the unit we are interested in depends strongly on\n",
    "the problem we are investigating. \n",
    "Traditionally, this was a report or an article.\n",
    "\n",
    "### We will refer to all these units of text as documents. \n",
    "It should be clear from the context what size a document has. Crucially, one document\n",
    "always represents one observation in our data. \n",
    "\n",
    "To refer to the entire collection of documents/observations, \n",
    "### we use the word corpus (plural corpora)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Data Preprocessing using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various Data Preprocessing methods are:\n",
    "\n",
    "Tokenization\n",
    "Frequency Distribution of Words\n",
    "Filtering Stop Words\n",
    "Stemming\n",
    "Lemmatization\n",
    "Parts of Speech(POS) Tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my family\n"
     ]
    }
   ],
   "source": [
    "# s.lower ()\n",
    "\n",
    "pre_str = 'I love my family'\n",
    "after_str = pre_str.lower()\n",
    "print (after_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Tokenization:\n",
    "\n",
    "Tokenization is the process by which a large quantity of text is divided into smaller parts called tokens. \n",
    "\n",
    "Natural Language toolkit has very important module NLTK tokenize sentences which further comprises of sub-modules\n",
    "\n",
    "+ word tokenize. Tokenizing your text by word allows you to identify words that come up particularly often. For example, if you were analyzing a group of job ads, then you might find that the word “Python” comes up often. That could suggest high demand for Python knowledge, but you’d need to look deeper to know more.\n",
    "\n",
    "+ sentence tokenize. When you tokenize by sentence, you can analyze how those words relate to one another and see more context. Are there a lot of negative words around the word “Python” because the hiring manager doesn’t like Python? Are there more terms from the domain of herpetology than the domain of software development, suggesting that you may be dealing with an entirely different kind of python than you were expecting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The voice that navigated was definitely that of a machine, \n",
    "and yet you could tell that the machine was a woman, which hurt my mind a little. \n",
    "How can machines have genders? The machine also had an American accent. \n",
    "How can machines have nationalities? This can't be a good idea, \n",
    "making machines talk like real people, can it? Giving machines humanoid identities?\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The voice that navigated was definitely that of a machine, \\nand yet you could tell that the machine was a woman, which hurt my mind a little.', 'How can machines have genders?', 'The machine also had an American accent.', 'How can machines have nationalities?', \"This can't be a good idea, \\nmaking machines talk like real people, can it?\", 'Giving machines humanoid identities?']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization. It helps when text data consists of multiple paragraphs.\n",
    "# import nltk \n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text_to_sentence = sent_tokenize(text)\n",
    "\n",
    "print(text_to_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'voice', 'that', 'navigated', 'was', 'definitely', 'that', 'of', 'a', 'machine', ',', 'and', 'yet', 'you', 'could', 'tell', 'that', 'the', 'machine', 'was', 'a', 'woman', ',', 'which', 'hurt', 'my', 'mind', 'a', 'little', '.', 'How', 'can', 'machines', 'have', 'genders', '?', 'The', 'machine', 'also', 'had', 'an', 'American', 'accent', '.', 'How', 'can', 'machines', 'have', 'nationalities', '?', 'This', 'ca', \"n't\", 'be', 'a', 'good', 'idea', ',', 'making', 'machines', 'talk', 'like', 'real', 'people', ',', 'can', 'it', '?', 'Giving', 'machines', 'humanoid', 'identities', '?']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word = word_tokenize(text)\n",
    "\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Frequency Distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 50 samples and 73 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "freq_dist_of_words = FreqDist(tokenized_word)\n",
    "\n",
    "print(freq_dist_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4), (',', 4), ('machines', 4), ('?', 4), ('that', 3)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most_common() is the function used to print the most frequent words.\n",
    "\n",
    "freq_dist_of_words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "freq_dist_of_words.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Removing Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples below, we will show how to remove stopwords from the string with NLTK. We first created “stopwords.word()” object with English vocabulary and stored the list of stopwords in a variable. Then we created an empty list to store words that are not stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spread love everywhere go. Let one ever come without leaving happier\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"Spread love everywhere you go. Let no one ever come to you without leaving happier\"\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "lst=[]\n",
    "for token in text.split():\n",
    "    if token.lower() not in en_stopwords:    #checking whether the word is not \n",
    "        lst.append(token)                    #present in the stopword list.\n",
    "        \n",
    "#Join items in the list\n",
    "print(' '.join(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Stop Words to Default NLTK Stopwords List\n",
    "\n",
    "we can add our own stopwords to the list of stopwords. To add a word to NLTK stop words list, we first create a list from the “stopwords.word(‘english’)” object. Next, we use the extend method on the list to add our list of words to the default stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords = stopwords.words('english')\n",
    "print(len(en_stopwords))\n",
    "\n",
    "new_stopwords = [\"you're\",\"i'll\",\"we'll\"]\n",
    "en_stopwords.extend(new_stopwords)\n",
    "\n",
    "len(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"you're\", \"i'll\", \"we'll\"]\n"
     ]
    }
   ],
   "source": [
    "print (en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Lemmatization\n",
    "\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of lemmatization:\n",
    "\n",
    "-> rocks : rock  \n",
    "\n",
    "-> corpora : corpus  \n",
    "\n",
    "-> better : good  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    " \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I           PRON      \t 4690420944186131903\tI\n",
      "am          AUX       \t10382539506755952630\tbe\n",
      "a           DET       \t11901859001352538922\ta\n",
      "runner      NOUN      \t12640964157389618806\trunner\n",
      "running     VERB      \t12767647472892411841\trun\n",
      "in          ADP       \t 3002984154512732771\tin\n",
      "a           DET       \t11901859001352538922\ta\n",
      "race        NOUN      \t 8048469955494714898\trace\n",
      "because     SCONJ     \t16950148841647037698\tbecause\n",
      "I           PRON      \t 4690420944186131903\tI\n",
      "love        VERB      \t 3702023516439754181\tlove\n",
      "to          PART      \t 3791531372978436496\tto\n",
      "run         VERB      \t12767647472892411841\trun\n",
      "since       SCONJ     \t10066841407251338481\tsince\n",
      "I           PRON      \t 4690420944186131903\tI\n",
      "ran         VERB      \t12767647472892411841\trun\n",
      "everyday    ADV       \t12502803309396265471\teveryday\n"
     ]
    }
   ],
   "source": [
    "text = nlp(u\"I am a runner running in a race because I love to run since I ran everyday\")\n",
    "\n",
    "for token in text:\n",
    "    print(f\"{token.text:{12}}{token.pos_:{10}}\\t{token.lemma:{20}}\\t{token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most popular stemming algorithms is the Porter stemmer, which has been around since 1979.  \n",
    "It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "\n",
    "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "by\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\kimuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON'),\n",
       " (\"'m\", 'VERB'),\n",
       " ('going', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('meet', 'VERB'),\n",
       " ('M.S', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Dhoni', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I'm going to meet M.S. Dhoni.\"\n",
    "tokenized_word = word_tokenize(text)\n",
    "\n",
    "nltk.pos_tag(tokenized_word, tagset='universal')\n",
    "text = \"I'm going to meet M.S. Dhoni.\"\n",
    "tokenized_word = word_tokenize(text)\n",
    "nltk.pos_tag(tokenized_word, tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
